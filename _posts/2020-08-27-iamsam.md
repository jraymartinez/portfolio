---
layout: single
header:
  teaser: /assets/images/iamsam.PNG
  image: /assets/images/iamsam_banner.png
title: "I AM SAM: An Automatic Text Summarization System using different Extractive Techniques"
date:   2020-08-27 16:50:00 +0800
categories: projects
mathjax: "true"
linkedin: "true"
tags: [text summarization, extractive technique, inormation retrieval, natural language processing]
excerpt: "This article covers the implementation of an automatic text summarization system called I AM SAM through Heroku, the system architecture, and features of the system including its capability to summarize news article from a URL."
---

## AUTHORS
[John Ray Martinez](https://jraymartinez.github.io/) (jbm332@drexel.edu), [Jonathan Musni](https://www.linkedin.com/in/jonathan-musni-624773134/) (jem472@drexel.edu), [Juan Miguel Trinidad](https://www.linkedin.com/in/miggytrinidad/) (jbt46@drexel.edu)

<sub> *This research is implemented in fulfillment of the requirements for the Information Retrieval Systems Course of Master of Science in Data Science under Drexel University College of Computing & Informatics* </sub>


## INTRODUCTION
In recent years, there has been a growth of large volume of text data from a variety of sources. This explosion of amount of text data led to the problem of information overload. The generation today called ‘Net generation’ learns through multitasking, performing activities simultaneously, and has short attention span. ‘Net generation’ can perform more tasks simultaneously and shift their attentions quickly from one to another, but would probably be overwhelmed if they are asked to read a long report. Thus, more educators motivate them to engage in the learning content by supplying shorter contents in the curricula [17].To alleviate information overload and considering the characteristic of the ‘Net generation’, the need for automatic text summarization is deemed necessary. 

One tool for text summarization is Python package sumy [1]. It has three most notably used models namely LSA (latent semantic analysis), LexRank and Luhn. LSA is an unsupervised method of summarization that combines term frequency techniques with singular value decomposition to summarize texts. Also an unsupervised approach, LexRank is a graphical based text summarizer inspired by algorithms PageRank. Meanwhile, Luhn is a naive approach based on TF-IDF. It scores sentences based on frequency of the most important words and also assigns higher weights to sentences occurring near the beginning of a document [2]. In this study, we investigate and evaluate the application of sumy models on the extractive summarization task using news articles and show that the results obtained with LSA are competitive with other two algorithms developed. 

Furthermore, utilizing the sumy extractive summarization techniques, we build and implement a web application on Heroku that mainly functions as text summarizer.

## EXPERIMENTS

### DATA DESCRIPTION
This dataset is approximately 2225 documents from the BBC
news website and represented into five topical areas such
as business, entertainment, politics, sport, and technology
[8]. This dataset for extractive text summarization has 510
business news articles of BBC from 2004 to 2005. For each
article, one summary are provided in the Summaries folder.
In this study, the first 100 pairs of business news articles
and its correponding reference summaries were manually
selected and used. The extractive summary articles will be
used as reference summaries (gold standard) for evaluating
the system summaries using ROUGE. 

### METHODOLOGY
We applied the three sumy methods in the sampled business news articles. All these algorithms extract six sentences from each article in order to compose the summary. We performed an experimental comparison with three extractive summarization techniques. The performance of each summarization technique was !['Algorithm 1']({{ site.url }}{{ site.baseurl }}/assets/images/iamsam_algo.png){: .image-left } evaluated by using variants of the ROUGE measure [11]. This performance metrics is a method based on Ngram statistics and found to be highly correlated with human evaluations [12]. Concretely, Rouge-N with unigrams and bigrams (Rouge1 and Rouge-2) and Rouge-L. Each Rouge has corresponding F1, precision, recall scores. First, the value of the evaluation measure was calculated for each of the article. Next, we took average of those scores to arrive at a consolidated Recall and F1 scores for each Rouge. Algorithm 1 shows the pesudo-code of the implementation of the method in this study.


## EXPERIMENTAL RESULTS AND DISCUSSION
We evaluate the four summarization techniques on a singledocument summarization task using 100 news articles from
business section of BBC dataset. For this task to have a
meaningful evaluation, we report ROUGE Recall as standard
evaluation and take output length into account [4]. For each
article, each summarizer generate a six-sentences summary.
The corresponding 100 human-created reference summaries
are provided by BBC and used in the evaluation process.
We compare the performance of the three different summarizing techniques with each other. Table 1 shows the results
obtained on this data set of 100 news articles, including the
results for LSA (shown in bold), and the results of the other
two sumy summarizers in the single document summarization
task.
LSA summarization technique succeeds in summarization
task on news articles followed by sumy-LexRank then sumyLuhn. Figure 1 Figure 1 visualizes the comparison of models
using Rouge Recall as performance metrics. As shown, LSA
has the best performance in extractive summarization task
on business news articles.

## IMPLEMENTED SYSTEM

### SYSTEM ARCHITECTURE
In this section, we present the overall architecture of our
proposed web application for single document summarization based on news components using sumy models LSA,
Luhn and LexRank. As highlighted in Figure 2, there are
three main phases which include the back-end, front-end,
and deployment. To create a web application, we utilized
Flask which is a micro web framework written in Python. For
layout to look good, we styled it with Boostrap. Finally, we
deployed the models on Heroku. Figure 3 visually presented
the detailed diagram of system features.

## CONCLUSION AND FUTURE WORKS
We presented sumy python package implementation of LSA(latent semantic analysis) outperforming the other models such as LexRank and Luhn in extractive summarization task using BBC business news articles. Furthermore, we implemented an automatic text summarization system called I AM SAM through Heroku that has capability to summarize news article from a URL or plain text utilizing the three sumy extractive summarization techniques.

As future work, we plan to extend the averaging algorithm to all articles in business news articles folder which has a total of 510 articles. In addition, we will explore other news articles such as entertainment, politics, sport, and technology.

## REFERENCES
[1] <a id='ref1'></a>M. Snider, Video games can be a healthy social pastime during coronavirus pandemic, USA Today, March 29, 2020. [Online]. Available: [https://www.usatoday.com/story/tech/gaming/2020/03/28/videogames-whos-prescription-solace-during-coronaviruspandemic/2932976001/](https://www.usatoday.com/story/tech/gaming/2020/03/28/videogames-whos-prescription-solace-during-coronaviruspandemic/2932976001/). [Accessed May 7, 2020].

[2] <a id="ref2"></a>P. Bertens, A. Guitart, P. P. Chen and A. Perianez, A Machine-Learning Item Recommendation System for Video Games, 2018 IEEE Conference on Computational Intelligence and Games (CIG), Maastricht, 2018, pp. 1-4, doi: 10.1109/CIG.2018.8490456.

[3] <a id="ref3"></a>O’Neill, M., Vaziripour, E., Wu, J., Zappala, D.: Condensing steam: distilling the diversity of gamer behavior. In Proceedings of the 2016 Internet Measurement Conference, IMC 2016, pp. 81-95. ACM, New York (2016). [https://doi.org/10.1145/2987443.2987489](https://doi.org/10.1145/2987443.2987489.).

[4] <a id="ref4"></a>Tamber Team. (2017, March). Steam Video Games. Retrieved May 10, 2020 from [https://www.kaggle.com/tamber/steam-video-games/](https://www.kaggle.com/tamber/steam-video-games/).

[5] <a id="ref5"></a>D. J. Prajapati, S. Garg and N. C. Chauhan," Interesting association rule mining with consistent and inconsistent rule detection from big sales data in distributed environment, " Future Computing and Informatics Journal, pp. 1-12, 2017.